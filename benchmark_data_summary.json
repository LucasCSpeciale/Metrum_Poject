{
  "GPT-4o Mini": {
    "Request Latency (ms)": {
      "avg": "2,235.71",
      "min": "831.24",
      "max": "7,729.99",
      "p99": "6,790.73",
      "p90": "3,045.73",
      "p75": "2,485.02"
    },
    "Output Sequence Length (tokens)": {
      "avg": "100.69",
      "min": "26.00",
      "max": "249.00",
      "p99": "211.00",
      "p90": "134.00",
      "p75": "113.25"
    },
    "Input Sequence Length (tokens)": {
      "avg": "50.00",
      "min": "50.00",
      "max": "50.00",
      "p99": "50.00",
      "p90": "50.00",
      "p75": "50.00"
    },
    "Metric": {
      "avg": "Value",
      "min": null,
      "max": null,
      "p99": null,
      "p90": null,
      "p75": null
    },
    "Output Token Throughput (per sec)": {
      "avg": "45.03",
      "min": null,
      "max": null,
      "p99": null,
      "p90": null,
      "p75": null
    },
    "Request Throughput (per sec)": {
      "avg": "0.45",
      "min": null,
      "max": null,
      "p99": null,
      "p90": null,
      "p75": null
    },
    "Request Count (count)": {
      "avg": "96.00",
      "min": null,
      "max": null,
      "p99": null,
      "p90": null,
      "p75": null
    }
  },
  "Claude 3 Haiku": {
    "Request Latency (ms)": {
      "avg": "2,177.63",
      "min": "957.45",
      "max": "7,033.93",
      "p99": "6,368.03",
      "p90": "2,790.51",
      "p75": "2,422.73"
    },
    "Output Sequence Length (tokens)": {
      "avg": "200.55",
      "min": "52.00",
      "max": "332.00",
      "p99": "307.50",
      "p90": "266.20",
      "p75": "245.50"
    },
    "Input Sequence Length (tokens)": {
      "avg": "50.00",
      "min": "50.00",
      "max": "50.00",
      "p99": "50.00",
      "p90": "50.00",
      "p75": "50.00"
    },
    "Metric": {
      "avg": "Value",
      "min": null,
      "max": null,
      "p99": null,
      "p90": null,
      "p75": null
    },
    "Output Token Throughput (per sec)": {
      "avg": "92.09",
      "min": null,
      "max": null,
      "p99": null,
      "p90": null,
      "p75": null
    },
    "Request Throughput (per sec)": {
      "avg": "0.46",
      "min": null,
      "max": null,
      "p99": null,
      "p90": null,
      "p75": null
    },
    "Request Count (count)": {
      "avg": "99.00",
      "min": null,
      "max": null,
      "p99": null,
      "p90": null,
      "p75": null
    }
  },
  "Llama 3.1 8B": {
    "Request Latency (ms)": {
      "avg": "5,494.07",
      "min": "410.94",
      "max": "72,947.47",
      "p99": "52,725.20",
      "p90": "7,595.38",
      "p75": "4,739.53"
    },
    "Output Sequence Length (tokens)": {
      "avg": "305.05",
      "min": "2.00",
      "max": "2,738.00",
      "p99": "2,147.86",
      "p90": "510.20",
      "p75": "308.00"
    },
    "Input Sequence Length (tokens)": {
      "avg": "50.00",
      "min": "50.00",
      "max": "50.00",
      "p99": "50.00",
      "p90": "50.00",
      "p75": "50.00"
    },
    "Metric": {
      "avg": "Value",
      "min": null,
      "max": null,
      "p99": null,
      "p90": null,
      "p75": null
    },
    "Output Token Throughput (per sec)": {
      "avg": "55.52",
      "min": null,
      "max": null,
      "p99": null,
      "p90": null,
      "p75": null
    },
    "Request Throughput (per sec)": {
      "avg": "0.18",
      "min": null,
      "max": null,
      "p99": null,
      "p90": null,
      "p75": null
    },
    "Request Count (count)": {
      "avg": "39.00",
      "min": null,
      "max": null,
      "p99": null,
      "p90": null,
      "p75": null
    }
  }
}
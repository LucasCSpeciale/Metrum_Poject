{
  "request_throughput": {
    "unit": "requests/sec",
    "avg": 0.182008364293515
  },
  "request_latency": {
    "unit": "ms",
    "avg": 5494.069679487179,
    "p25": 1250.6690214999999,
    "p50": 2613.880918,
    "p75": 4739.5302105,
    "p90": 7595.3794342,
    "p95": 18281.2778031,
    "p99": 52725.20278623987,
    "min": 410.940251,
    "max": 72947.471381,
    "std": 11685.453548773368
  },
  "request_count": {
    "unit": "count",
    "avg": 39.0
  },
  "time_to_first_token": {
    "unit": "ms",
    "avg": 5494.069679487179,
    "p25": 1250.6690214999999,
    "p50": 2613.880918,
    "p75": 4739.5302105,
    "p90": 7595.3794342,
    "p95": 18281.2778031,
    "p99": 52725.20278623987,
    "min": 410.940251,
    "max": 72947.471381,
    "std": 11685.453548773368
  },
  "output_token_throughput": {
    "unit": "tokens/sec",
    "avg": 55.52188487179353
  },
  "output_token_throughput_per_request": {
    "unit": "tokens/sec",
    "avg": 79.78830389731574,
    "p25": 39.246019531060234,
    "p50": 55.125295883647524,
    "p75": 97.59357832292773,
    "p90": 167.68937332314232,
    "p95": 198.280224240358,
    "p99": 234.31389109282188,
    "min": 1.9716627153329993,
    "max": 251.15105938997087,
    "std": 57.88592376265582
  },
  "output_sequence_length": {
    "unit": "tokens",
    "avg": 305.05128205128204,
    "p25": 105.0,
    "p50": 160.0,
    "p75": 308.0,
    "p90": 510.20000000000016,
    "p95": 1066.2000000000003,
    "p99": 2147.859999999996,
    "min": 2.0,
    "max": 2738.0,
    "std": 462.6471185952746
  },
  "input_sequence_length": {
    "unit": "tokens",
    "avg": 50.0,
    "p25": 50.0,
    "p50": 50.0,
    "p75": 50.0,
    "p90": 50.0,
    "p95": 50.0,
    "p99": 50.0,
    "min": 50.0,
    "max": 50.0,
    "std": 0.0
  },
  "input_config": {
    "subcommand": "profile",
    "model": [
      "meta-llama/llama-3.1-8b-instruct"
    ],
    "model_selection_strategy": "round_robin",
    "backend": "tensorrtllm",
    "endpoint": "v1/chat/completions",
    "endpoint_type": "chat",
    "service_kind": "openai",
    "server_metrics_url": null,
    "streaming": false,
    "u": "https://openrouter.ai/api",
    "image_width_mean": 100,
    "image_width_stddev": 0,
    "image_height_mean": 100,
    "image_height_stddev": 0,
    "image_format": null,
    "batch_size_image": 1,
    "batch_size_text": 1,
    "goodput": null,
    "header": [
      "Authorization:Bearer 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",
      "HTTP-Referer:http://localhost:8000",
      "X-Title:GenAI-Perf-Benchmark"
    ],
    "num_dataset_entries": 10,
    "num_prefix_prompts": 0,
    "output_tokens_mean": -1,
    "output_tokens_mean_deterministic": false,
    "output_tokens_stddev": 0,
    "random_seed": 0,
    "request_count": 0,
    "synthetic_input_tokens_mean": 50,
    "synthetic_input_tokens_stddev": 0,
    "prefix_prompt_length": 100,
    "warmup_request_count": 0,
    "verbose": false,
    "artifact_dir": "/workspace/results/llama-3.1-8b",
    "generate_plots": false,
    "profile_export_file": "/workspace/results/llama-3.1-8b/profile_export.json",
    "concurrency": 1,
    "measurement_interval": 60000,
    "request_rate": null,
    "stability_percentage": 999,
    "tokenizer": "gpt2",
    "tokenizer_revision": "main",
    "tokenizer_trust_remote_code": false,
    "synthetic_input_files": null,
    "prompt_source": "synthetic",
    "formatted_model_name": "meta-llama/llama-3.1-8b-instruct",
    "extra_inputs": {}
  }
}